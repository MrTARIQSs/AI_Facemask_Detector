{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Models_Arch_ICS381.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1fnNzGoBNx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8b06dde-7d13-4527-dd9d-8a1dbe7396b3"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.models as models\n",
        "!pip install adabelief-pytorch==0.1.0\n",
        "from adabelief_pytorch import AdaBelief\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "import torchvision   \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import natsort\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import accuracy_score\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting adabelief-pytorch==0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e2/dd/4c09c94513bcef38de13d82e9c05478e3f6856c97c753693990d29808c63/adabelief_pytorch-0.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from adabelief-pytorch==0.1.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: tabulate>=0.7 in /usr/local/lib/python3.6/dist-packages (from adabelief-pytorch==0.1.0) (0.8.7)\n",
            "Collecting colorama>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.0->adabelief-pytorch==0.1.0) (1.18.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.0->adabelief-pytorch==0.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.0->adabelief-pytorch==0.1.0) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.0->adabelief-pytorch==0.1.0) (0.8)\n",
            "Installing collected packages: colorama, adabelief-pytorch\n",
            "Successfully installed adabelief-pytorch-0.1.0 colorama-0.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c4cCvDF_W7S",
        "outputId": "2910ede8-bdde-4963-c4d1-1af34fe5328e"
      },
      "source": [
        "!git clone https://github.com/prajnasb/observations.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'observations'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 1638 (delta 9), reused 0 (delta 0), pack-reused 1604\u001b[K\n",
            "Receiving objects: 100% (1638/1638), 75.94 MiB | 8.77 MiB/s, done.\n",
            "Resolving deltas: 100% (20/20), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZv6vhcs_EyK"
      },
      "source": [
        "data_transform = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize((224,224)),                                        \n",
        "        torchvision.transforms.RandomAffine(20,(0.2,0.2),None,0.15,0,0),\n",
        "        torchvision.transforms.RandomHorizontalFlip(),\n",
        "        torchvision.transforms.ToTensor()\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2SEVfuTjB2C"
      },
      "source": [
        "train_dataset = torchvision.datasets.ImageFolder(root='/content/observations/experiements/dest_folder/train/', \n",
        "                                                 transform=data_transform)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, \n",
        "                                               shuffle=True, num_workers=24)\n",
        "\n",
        "val_dataset = torchvision.datasets.ImageFolder(root='/content/observations/experiements/dest_folder/val/', \n",
        "                                               transform=data_transform)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, \n",
        "                                             shuffle=False, num_workers=24)\n",
        "test_dataset = torchvision.datasets.ImageFolder(root='/content/observations/experiements/dest_folder/test/', \n",
        "                                               transform=data_transform)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, \n",
        "                                             shuffle=False, num_workers=24)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1xEP63qEjcW"
      },
      "source": [
        "def train(model, data_loader, test_loader, task='Classification'):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(numEpochs):\n",
        "        avg_loss = 0.0\n",
        "        for batch_num, (feats, labels) in enumerate(data_loader):\n",
        "            feats, labels = feats.to(device), labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(feats)\n",
        "\n",
        "            loss = criterion(outputs, labels.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            avg_loss += loss.item()\n",
        "\n",
        "            if batch_num % 50 == 49:\n",
        "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
        "                avg_loss = 0.0    \n",
        "            \n",
        "            torch.cuda.empty_cache()\n",
        "            del feats\n",
        "            del labels\n",
        "            del loss\n",
        "        \n",
        "        if task == 'Classification':\n",
        "            val_loss, val_acc = test_classify(model, test_loader)\n",
        "            train_loss, train_acc = test_classify(model, data_loader)\n",
        "            print('Epoch: {}\\tTrain Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\tVal Loss: {:.4f}\\tVal Accuracy: {:.4f}'.\n",
        "                  format(epoch+1,train_loss, train_acc, val_loss, val_acc))\n",
        "        else:\n",
        "            test_verify(model, test_loader)\n",
        "\n",
        "\n",
        "def test_classify(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = []\n",
        "    accuracy = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_num, (feats, labels) in enumerate(test_loader):\n",
        "        feats, labels = feats.to(device), labels.to(device)\n",
        "        outputs = model(feats)\n",
        "        \n",
        "        _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
        "        pred_labels = pred_labels.view(-1)\n",
        "        \n",
        "        loss = criterion(outputs, labels.long())\n",
        "        \n",
        "        accuracy += torch.sum(torch.eq(pred_labels, labels)).item()\n",
        "        total += len(labels)\n",
        "        test_loss.extend([loss.item()]*feats.size()[0])\n",
        "        del feats\n",
        "        del labels\n",
        "\n",
        "    model.train()\n",
        "    return np.mean(test_loss), accuracy/total\n",
        "def init_xavier(m):\n",
        "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_normal_(m.weight.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9-UXBjXAmUz"
      },
      "source": [
        "class Net(nn.Module):   \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.cnn_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(output_size=(1,1)),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        flatten = self.cnn_layers(torch.rand(1, 3, 224, 224)).view(-1).size(0)\n",
        "        self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(flatten, 512),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 2)\n",
        "        )\n",
        "\n",
        "    # Defining the forward pass    \n",
        "    def forward(self, x):\n",
        "        x = self.cnn_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear_layers(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT1PkrIGGGiP",
        "outputId": "77e5d586-3140-40be-f742-887d1eb4fe25"
      },
      "source": [
        "network = Net()\n",
        "print(network)\n",
        "network.apply(init_xavier)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer =  AdaBelief(network.parameters(), lr=1e-3, eps=1e-8, betas=(0.9,0.999), weight_decouple = True, rectify = False, weight_decay=1e-2,fixed_decay=False,amsgrad=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (cnn_layers): Sequential(\n",
            "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU(inplace=True)\n",
            "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (12): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (14): ReLU(inplace=True)\n",
            "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (16): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (20): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (linear_layers): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
            "    (1): Dropout(p=0.2, inplace=False)\n",
            "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (3): Dropout(p=0.2, inplace=False)\n",
            "    (4): Linear(in_features=256, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
            "\u001b[31mModifications to default arguments:\n",
            "\u001b[31m                           eps  weight_decouple    rectify\n",
            "-----------------------  -----  -----------------  ---------\n",
            "adabelief-pytorch=0.0.5  1e-08  False              False\n",
            "Current version (0.1.0)  1e-16  True               True\n",
            "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
            "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
            "\u001b[0m\n",
            "Weight decoupling enabled in AdaBelief\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD9QSv70Gaxn",
        "outputId": "2b238599-71c9-40a7-e321-9f0d29c12b2f"
      },
      "source": [
        "numEpochs = 20\n",
        "network.train()\n",
        "network.to(device)\n",
        "train(network, train_dataloader, val_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\tTrain Loss: 3.0333\tTrain Accuracy: 0.4996\tVal Loss: 3.0186\tVal Accuracy: 0.5000\n",
            "Epoch: 2\tTrain Loss: 0.2502\tTrain Accuracy: 0.9202\tVal Loss: 0.1564\tVal Accuracy: 0.9225\n",
            "Epoch: 3\tTrain Loss: 0.1432\tTrain Accuracy: 0.9658\tVal Loss: 0.0635\tVal Accuracy: 0.9718\n",
            "Epoch: 4\tTrain Loss: 0.1862\tTrain Accuracy: 0.9346\tVal Loss: 0.1151\tVal Accuracy: 0.9437\n",
            "Epoch: 5\tTrain Loss: 1.4125\tTrain Accuracy: 0.5703\tVal Loss: 1.0924\tVal Accuracy: 0.6127\n",
            "Epoch: 6\tTrain Loss: 0.1464\tTrain Accuracy: 0.9536\tVal Loss: 0.0226\tVal Accuracy: 0.9930\n",
            "Epoch: 7\tTrain Loss: 0.0696\tTrain Accuracy: 0.9810\tVal Loss: 0.0328\tVal Accuracy: 0.9789\n",
            "Epoch: 8\tTrain Loss: 0.0670\tTrain Accuracy: 0.9787\tVal Loss: 0.0238\tVal Accuracy: 0.9930\n",
            "Epoch: 9\tTrain Loss: 0.0523\tTrain Accuracy: 0.9833\tVal Loss: 0.0132\tVal Accuracy: 1.0000\n",
            "Epoch: 10\tTrain Loss: 0.2190\tTrain Accuracy: 0.9156\tVal Loss: 0.1231\tVal Accuracy: 0.9577\n",
            "Epoch: 11\tTrain Loss: 0.4833\tTrain Accuracy: 0.9042\tVal Loss: 0.4995\tVal Accuracy: 0.9366\n",
            "Epoch: 12\tTrain Loss: 0.6836\tTrain Accuracy: 0.8631\tVal Loss: 0.6065\tVal Accuracy: 0.8380\n",
            "Epoch: 13\tTrain Loss: 0.0518\tTrain Accuracy: 0.9787\tVal Loss: 0.0128\tVal Accuracy: 1.0000\n",
            "Epoch: 14\tTrain Loss: 0.0846\tTrain Accuracy: 0.9597\tVal Loss: 0.0218\tVal Accuracy: 1.0000\n",
            "Epoch: 15\tTrain Loss: 0.0307\tTrain Accuracy: 0.9909\tVal Loss: 0.0091\tVal Accuracy: 1.0000\n",
            "Epoch: 16\tTrain Loss: 0.0896\tTrain Accuracy: 0.9711\tVal Loss: 0.0661\tVal Accuracy: 0.9930\n",
            "Epoch: 17\tTrain Loss: 0.0459\tTrain Accuracy: 0.9817\tVal Loss: 0.0315\tVal Accuracy: 0.9930\n",
            "Epoch: 18\tTrain Loss: 0.1224\tTrain Accuracy: 0.9506\tVal Loss: 0.0716\tVal Accuracy: 0.9789\n",
            "Epoch: 19\tTrain Loss: 0.0716\tTrain Accuracy: 0.9719\tVal Loss: 0.0871\tVal Accuracy: 0.9648\n",
            "Epoch: 20\tTrain Loss: 0.2875\tTrain Accuracy: 0.9384\tVal Loss: 0.3295\tVal Accuracy: 0.9507\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEkH7c9JKKC8"
      },
      "source": [
        "#The results of this network\n",
        "            self.cnn_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(128*26*26, 512),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.Linear(256, 2)\n",
        "        )\n",
        "#are :\n",
        "Epoch: 1\tTrain Loss: 28.7583\tTrain Accuracy: 0.4996\tVal Loss: 26.2200\tVal Accuracy: 0.5000\n",
        "\n",
        "Epoch: 2\tTrain Loss: 2.4462\tTrain Accuracy: 0.8738\tVal Loss: 0.8310\tVal Accuracy: 0.8944\n",
        "\n",
        "Epoch: 3\tTrain Loss: 1.5042\tTrain Accuracy: 0.7559\tVal Loss: 1.1459\tVal Accuracy: 0.8169\n",
        "\n",
        "Epoch: 4\tTrain Loss: 4.3187\tTrain Accuracy: 0.8829\tVal Loss: 2.8687\tVal Accuracy: 0.9085\n",
        "\n",
        "Epoch: 5\tTrain Loss: 0.6918\tTrain Accuracy: 0.8700\tVal Loss: 0.5393\tVal Accuracy: 0.9085\n",
        "\n",
        "Epoch: 6\tTrain Loss: 0.1974\tTrain Accuracy: 0.9285\tVal Loss: 0.0879\tVal Accuracy: 0.9648\n",
        "\n",
        "Epoch: 7\tTrain Loss: 0.2421\tTrain Accuracy: 0.9262\tVal Loss: 0.1064\tVal Accuracy: 0.9648\n",
        "\n",
        "Epoch: 8\tTrain Loss: 0.1956\tTrain Accuracy: 0.9354\tVal Loss: 0.0992\tVal Accuracy: 0.9577\n",
        "\n",
        "Epoch: 9\tTrain Loss: 0.1959\tTrain Accuracy: 0.9521\tVal Loss: 0.1169\tVal Accuracy: 0.9577\n",
        "\n",
        "Epoch: 10\tTrain Loss: 0.1911\tTrain Accuracy: 0.9361\tVal Loss: 0.1109\tVal Accuracy: 0.9648\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4w5W9opN78E"
      },
      "source": [
        "#The results of this network            \n",
        "            self.cnn_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        flatten = self.cnn_layers(torch.rand(1, 3, 224, 224)).view(-1).size(0)\n",
        "        self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(flatten, 512),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.Linear(256, 2)\n",
        "        )\n",
        "#are : \n",
        "Epoch: 1\tTrain Loss: 3.4300\tTrain Accuracy: 0.4996\tVal Loss: 3.4068\tVal Accuracy: 0.5000\n",
        "\n",
        "Epoch: 2\tTrain Loss: 0.7142\tTrain Accuracy: 0.5414\tVal Loss: 0.7142\tVal Accuracy: 0.5211\n",
        "\n",
        "Epoch: 3\tTrain Loss: 0.2586\tTrain Accuracy: 0.9316\tVal Loss: 0.0856\tVal Accuracy: 0.9718\n",
        "\n",
        "Epoch: 4\tTrain Loss: 0.2069\tTrain Accuracy: 0.9293\tVal Loss: 0.0955\tVal Accuracy: 0.9718\n",
        "\n",
        "Epoch: 5\tTrain Loss: 0.1440\tTrain Accuracy: 0.9475\tVal Loss: 0.0986\tVal Accuracy: 0.9648\n",
        "\n",
        "Epoch: 6\tTrain Loss: 0.1119\tTrain Accuracy: 0.9597\tVal Loss: 0.0307\tVal Accuracy: 0.9930\n",
        "\n",
        "Epoch: 7\tTrain Loss: 0.3444\tTrain Accuracy: 0.8502\tVal Loss: 0.2988\tVal Accuracy: 0.8592\n",
        "\n",
        "Epoch: 8\tTrain Loss: 0.1023\tTrain Accuracy: 0.9627\tVal Loss: 0.0425\tVal Accuracy: 0.9648\n",
        "\n",
        "Epoch: 9\tTrain Loss: 0.1170\tTrain Accuracy: 0.9559\tVal Loss: 0.0261\tVal Accuracy: 0.9930\n",
        "\n",
        "Epoch: 10\tTrain Loss: 0.0862\tTrain Accuracy: 0.9757\tVal Loss: 0.0393\tVal Accuracy: 0.9859\n",
        "\n",
        "Epoch: 11\tTrain Loss: 0.1210\tTrain Accuracy: 0.9620\tVal Loss: 0.0472\tVal Accuracy: 0.9718\n",
        "\n",
        "Epoch: 12\tTrain Loss: 0.1233\tTrain Accuracy: 0.9681\tVal Loss: 0.0361\tVal Accuracy: 0.9930\n",
        "\n",
        "Epoch: 13\tTrain Loss: 1.2096\tTrain Accuracy: 0.5559\tVal Loss: 1.1720\tVal Accuracy: 0.5775\n",
        "\n",
        "Epoch: 14\tTrain Loss: 0.0783\tTrain Accuracy: 0.9658\tVal Loss: 0.0403\tVal Accuracy: 0.9859\n",
        "\n",
        "Epoch: 15\tTrain Loss: 0.1185\tTrain Accuracy: 0.9605\tVal Loss: 0.0341\tVal Accuracy: 0.9859\n",
        "\n",
        "Epoch: 16\tTrain Loss: 0.0794\tTrain Accuracy: 0.9741\tVal Loss: 0.0232\tVal Accuracy: 0.9859\n",
        "\n",
        "Epoch: 17\tTrain Loss: 0.1391\tTrain Accuracy: 0.9559\tVal Loss: 0.0521\tVal Accuracy: 0.9718\n",
        "\n",
        "Epoch: 18\tTrain Loss: 0.1867\tTrain Accuracy: 0.9483\tVal Loss: 0.1596\tVal Accuracy: 0.9296\n",
        "\n",
        "Epoch: 19\tTrain Loss: 0.1882\tTrain Accuracy: 0.9194\tVal Loss: 0.0730\tVal Accuracy: 0.9789\n",
        "\n",
        "Epoch: 20\tTrain Loss: 0.4546\tTrain Accuracy: 0.8175\tVal Loss: 0.3714\tVal Accuracy: 0.8451"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVbbNEVhQ0vc"
      },
      "source": [
        "#The results of this network(Best one so far)          \n",
        "            self.cnn_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(output_size=(1,1)),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        flatten = self.cnn_layers(torch.rand(1, 3, 224, 224)).view(-1).size(0)\n",
        "        self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(flatten, 512),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 2)\n",
        "        )\n",
        "#are :\n",
        "Epoch: 1\tTrain Loss: 3.0333\tTrain Accuracy: 0.4996\tVal Loss: 3.0186\tVal Accuracy: 0.5000\n",
        "\n",
        "Epoch: 2\tTrain Loss: 0.2502\tTrain Accuracy: 0.9202\tVal Loss: 0.1564\tVal Accuracy: 0.9225\n",
        "\n",
        "Epoch: 3\tTrain Loss: 0.1432\tTrain Accuracy: 0.9658\tVal Loss: 0.0635\tVal Accuracy: 0.9718\n",
        "\n",
        "Epoch: 4\tTrain Loss: 0.1862\tTrain Accuracy: 0.9346\tVal Loss: 0.1151\tVal Accuracy: 0.9437\n",
        "\n",
        "Epoch: 5\tTrain Loss: 1.4125\tTrain Accuracy: 0.5703\tVal Loss: 1.0924\tVal Accuracy: 0.6127\n",
        "\n",
        "Epoch: 6\tTrain Loss: 0.1464\tTrain Accuracy: 0.9536\tVal Loss: 0.0226\tVal Accuracy: 0.9930\n",
        "\n",
        "Epoch: 7\tTrain Loss: 0.0696\tTrain Accuracy: 0.9810\tVal Loss: 0.0328\tVal Accuracy: 0.9789\n",
        "\n",
        "Epoch: 8\tTrain Loss: 0.0670\tTrain Accuracy: 0.9787\tVal Loss: 0.0238\tVal Accuracy: 0.9930\n",
        "\n",
        "Epoch: 9\tTrain Loss: 0.0523\tTrain Accuracy: 0.9833\tVal Loss: 0.0132\tVal Accuracy: 1.0000\n",
        "\n",
        "Epoch: 10\tTrain Loss: 0.2190\tTrain Accuracy: 0.9156\tVal Loss: 0.1231\tVal Accuracy: 0.9577\n",
        "\n",
        "Epoch: 11\tTrain Loss: 0.4833\tTrain Accuracy: 0.9042\tVal Loss: 0.4995\tVal Accuracy: 0.9366\n",
        "\n",
        "Epoch: 12\tTrain Loss: 0.6836\tTrain Accuracy: 0.8631\tVal Loss: 0.6065\tVal Accuracy: 0.8380\n",
        "\n",
        "Epoch: 13\tTrain Loss: 0.0518\tTrain Accuracy: 0.9787\tVal Loss: 0.0128\tVal Accuracy: 1.0000\n",
        "\n",
        "Epoch: 14\tTrain Loss: 0.0846\tTrain Accuracy: 0.9597\tVal Loss: 0.0218\tVal Accuracy: 1.0000\n",
        "\n",
        "Epoch: 15\tTrain Loss: 0.0307\tTrain Accuracy: 0.9909\tVal Loss: 0.0091\tVal Accuracy: 1.0000\n",
        "\n",
        "Epoch: 16\tTrain Loss: 0.0896\tTrain Accuracy: 0.9711\tVal Loss: 0.0661\tVal Accuracy: 0.9930\n",
        "\n",
        "Epoch: 17\tTrain Loss: 0.0459\tTrain Accuracy: 0.9817\tVal Loss: 0.0315\tVal Accuracy: 0.9930\n",
        "\n",
        "Epoch: 18\tTrain Loss: 0.1224\tTrain Accuracy: 0.9506\tVal Loss: 0.0716\tVal Accuracy: 0.9789\n",
        "\n",
        "Epoch: 19\tTrain Loss: 0.0716\tTrain Accuracy: 0.9719\tVal Loss: 0.0871\tVal Accuracy: 0.9648\n",
        "\n",
        "Epoch: 20\tTrain Loss: 0.2875\tTrain Accuracy: 0.9384\tVal Loss: 0.3295\tVal Accuracy: 0.9507"
      ]
    }
  ]
}